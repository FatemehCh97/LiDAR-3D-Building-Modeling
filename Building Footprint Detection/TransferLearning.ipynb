{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBF_CvB55dQF"
      },
      "source": [
        "# Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqsL-QQYqO9P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import datetime\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from tqdm import notebook, tnrange\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SW7xIkzoKiI"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_unet_collection\n",
        "from keras_unet_collection import models, utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39FzQ3Ypb5UM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.losses import *\n",
        "from keras_unet_collection.losses import iou_seg\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTLsqmdtDccx"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VgZuJawDccx"
      },
      "outputs": [],
      "source": [
        "dataset_dir = \"Amsterdam Training Data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGzVE17IDccx"
      },
      "source": [
        "List of Train Data names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7H8tEnnDccx"
      },
      "outputs": [],
      "source": [
        "img_id = [] # list of names all images in the given path\n",
        "for f in glob.glob(os.path.join(dataset_dir,\"Train\", \"images\", \"*.tif\")):\n",
        "    img_id.append(os.path.split(f)[1].split(\".\")[0])\n",
        "img_id.sort()\n",
        "\n",
        "label_id = []\n",
        "for f in glob.glob(os.path.join(dataset_dir,\"Train\", \"labels\", \"*.tif\")):\n",
        "    label_id.append(os.path.split(f)[1].split(\".\")[0])\n",
        "label_id.sort()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmQ6fPZARYDV"
      },
      "source": [
        "List of test data names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpJo0-dSRWyf"
      },
      "outputs": [],
      "source": [
        "test_img_id = []\n",
        "for f in glob.glob(os.path.join(dataset_dir,\"Test\", \"images\", \"*.tif\")):\n",
        "    test_img_id.append(os.path.split(f)[1].split(\".\")[0])\n",
        "test_img_id.sort()\n",
        "\n",
        "test_label_id = []\n",
        "for f in glob.glob(os.path.join(dataset_dir,,\"Test\", \"labels\", \"*.tif\")):\n",
        "    test_label_id.append(os.path.split(f)[1].split(\".\")[0])\n",
        "test_label_id.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx_Gq9LSDccx"
      },
      "outputs": [],
      "source": [
        "im_width = 512\n",
        "im_height = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3gGXqMIDccy"
      },
      "outputs": [],
      "source": [
        "X = np.zeros((len(img_id), im_height, im_width, 1), dtype=np.float32)\n",
        "y = np.zeros((len(img_id), im_height, im_width, 1), dtype=np.float32)\n",
        "\n",
        "X_test = np.zeros((len(test_img_id), im_height, im_width, 1), dtype=np.float32)\n",
        "y_test = np.zeros((len(test_img_id), im_height, im_width, 1), dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKgowNpMPW3e"
      },
      "source": [
        "Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zhOFF9mDccy"
      },
      "outputs": [],
      "source": [
        "for n, id_ in notebook.tqdm(enumerate(img_id), total=len(img_id)):\n",
        "    # Load images\n",
        "    img = load_img(dataset_dir+\"/images/\"+id_+\".tif\", grayscale=True)\n",
        "    x_img = img_to_array(img)\n",
        "    # Load masks\n",
        "    mask = img_to_array(load_img(dataset_dir+\"/labels/\"+id_+\".tif\", grayscale=True))\n",
        "    # Normalization\n",
        "    X[n] = (x_img - x_img.min()) / (x_img.max() - x_img.min())\n",
        "    # y[n] = (mask - mask.min()) / (mask.max() - mask.min())\n",
        "    y[n] = mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF-BVHCRPa18"
      },
      "source": [
        "Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1h4CbbePmZe"
      },
      "outputs": [],
      "source": [
        "for n, id_ in notebook.tqdm(enumerate(test_img_id), total=len(test_img_id)):\n",
        "    img = load_img(dataset_dir+\"/images/\"+id_+\".tif\", grayscale=True)\n",
        "    x_img = img_to_array(img)\n",
        "    mask = img_to_array(load_img(dataset_dir+\"/labels/\"+id_+\".tif\", grayscale=True))\n",
        "    X_test[n] = (x_img - x_img.min()) / (x_img.max() - x_img.min())\n",
        "    # y[n] = (mask - mask.min()) / (mask.max() - mask.min())\n",
        "    y_test[n] = mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kEhG2FlDccz"
      },
      "source": [
        "Split validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzZQ1hYKDccz"
      },
      "outputs": [],
      "source": [
        "# Split train and valid\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=8, shuffle=True)\n",
        "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1665, random_state=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-HXsWNzI0aV"
      },
      "outputs": [],
      "source": [
        "print(np.shape(X_train))\n",
        "print(np.shape(X_valid))\n",
        "print(np.shape(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP90CTBozXxy"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoH_AsWezXxy"
      },
      "outputs": [],
      "source": [
        "img_data_gen_args = dict(rotation_range=60,\n",
        "                        width_shift_range=0.3,\n",
        "                        height_shift_range=0.3,\n",
        "                        shear_range=0.5,\n",
        "                        horizontal_flip=True,\n",
        "                        zoom_range=0.2,\n",
        "                        fill_mode='reflect')\n",
        "\n",
        "mask_data_gen_args = dict(rotation_range=60,\n",
        "                        width_shift_range=0.3,\n",
        "                        height_shift_range=0.3,\n",
        "                        shear_range=0.5,\n",
        "                        horizontal_flip=True,\n",
        "                        zoom_range=0.2,\n",
        "                        fill_mode='reflect',\n",
        "                        preprocessing_function = lambda x: np.where(x>0, 1, 0).astype(x.dtype)) #Binarize the output again.\n",
        "\n",
        "image_data_generator = ImageDataGenerator(**img_data_gen_args)\n",
        "\n",
        "# batch_size= 8 # Attention U-Net\n",
        "batch_size= 4 # U-Net3+\n",
        "seed = 24\n",
        "\n",
        "image_generator = image_data_generator.flow(X_train, batch_size=batch_size, seed=seed)\n",
        "# valid_img_generator = image_data_generator.flow(X_valid, batch_size=batch_size)\n",
        "\n",
        "mask_data_generator = ImageDataGenerator(**mask_data_gen_args)\n",
        "#mask_data_generator.fit(y_train, augment=True, seed=seed)\n",
        "mask_generator = mask_data_generator.flow(y_train, batch_size=batch_size, seed=seed)\n",
        "# valid_mask_generator = mask_data_generator.flow(y_valid, batch_size=batch_size)\n",
        "\n",
        "\n",
        "def my_image_mask_generator(image_generator, mask_generator):\n",
        "    train_generator = zip(image_generator, mask_generator)\n",
        "    for (img, mask) in train_generator:\n",
        "        yield (img, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjdSdoRJzXxy"
      },
      "outputs": [],
      "source": [
        "train = my_image_mask_generator(image_generator, mask_generator)\n",
        "\n",
        "# validation = my_image_mask_generator(valid_img_generator, valid_mask_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGLIPIpdnRtV"
      },
      "source": [
        "# Plot Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPx2q1l8nRtV"
      },
      "outputs": [],
      "source": [
        "# visualize samples and their predicted label\n",
        "def plot_sample(X, y, preds, binary_preds, ix=None):\n",
        "    if ix is None:\n",
        "        ix = random.randint(0, len(X))\n",
        "\n",
        "    has_mask = y[ix].max() > 0\n",
        "\n",
        "    fig, ax = plt.subplots(1, 4, figsize=(20, 10))\n",
        "    ax[0].imshow(X[ix, ..., 0], cmap='seismic')\n",
        "    if has_mask:\n",
        "        ax[0].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
        "    ax[0].set_title('Images')\n",
        "\n",
        "    ax[1].imshow(y[ix].squeeze(), cmap='gray')\n",
        "    ax[1].set_title('Ground Truth Building footprint')\n",
        "\n",
        "    ax[2].imshow(preds[ix].squeeze(), cmap='gray', vmin=0, vmax=1)\n",
        "    if has_mask:\n",
        "        ax[2].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
        "    ax[2].set_title('Predicted Building Footprint')\n",
        "\n",
        "    ax[3].imshow(binary_preds[ix].squeeze(), cmap='gray', vmin=0, vmax=1)\n",
        "    # if has_mask:\n",
        "    #     ax[3].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
        "    ax[3].set_title('Binary Predicted Building Footprint')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JusO64ANoKiI"
      },
      "source": [
        "# Load Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIuRHnzpFbB3"
      },
      "source": [
        "The top-performing models trained on Miami-Dade data were selected for fine-tuning using transfer learning techniques with Amsterdam data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsqfplv6i2o_"
      },
      "source": [
        "## U-Net3+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9b8aP4-FamZ"
      },
      "outputs": [],
      "source": [
        "model_unet3p_path = os.path.join(\"Model Output\", \"U-Net3+\", \"unet3p_model_1.h5\")\n",
        "\n",
        "with tf.device('/CPU:0'):\n",
        "    unet3p_model = load_model(model_unet3p_path, compile=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMH7ITkfiVA-"
      },
      "outputs": [],
      "source": [
        "# unet3+ loss/compile\n",
        "def hybrid_loss(y_true, y_pred):\n",
        "\n",
        "    loss_focal = losses.focal_tversky(y_true, y_pred, alpha=0.5, gamma=4/3)\n",
        "    loss_iou = losses.iou_seg(y_true, y_pred)\n",
        "\n",
        "    # (x)\n",
        "    #loss_ssim = losses.ms_ssim(y_true, y_pred, max_val=1.0, filter_size=4)\n",
        "\n",
        "    return loss_focal+loss_iou #+loss_ssim\n",
        "\n",
        "unet3p_model.compile(loss=[hybrid_loss, hybrid_loss, hybrid_loss, hybrid_loss, hybrid_loss],\n",
        "                          loss_weights=[0.25, 0.25, 0.25, 0.25, 1.0],\n",
        "                          optimizer=keras.optimizers.Adam(learning_rate=1e-4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_dir = os.path.join(\"Model Output\", \"TransferLearning\")\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "logdir = os.path.join(base_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "with tf.device('/CPU:0'):\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.CSVLogger(os.path.join(base_dir, \"unet3p_amst.csv\"), append=True, separator=';'),\n",
        "        ModelCheckpoint(\n",
        "            filepath=os.path.join(base_dir, \"unet3p_amst.h5\"),\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1,\n",
        "            save_best_only=True\n",
        "        ),\n",
        "        TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKziv4ogiLr1"
      },
      "outputs": [],
      "source": [
        "total_training_sample = np.shape(X_train)[0]\n",
        "batch_size = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nel3FB_ykv2I"
      },
      "outputs": [],
      "source": [
        "with tf.device('/GPU:0'):\n",
        "    history_u3 = unet3p_model.fit(train, steps_per_epoch=round(total_training_sample/batch_size), epochs=75, batch_size=4 , validation_data = (X_valid, y_valid), callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x85vjlBfseZ"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAxw9b3eh032"
      },
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "unet3p_model.evaluate(X_train, y_train, verbose=1, batch_size=4)\n",
        "unet3p_model.evaluate(X_valid, y_valid, verbose=1, batch_size=4)\n",
        "unet3p_model.evaluate(X_test, y_test, verbose=1, batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUwpGZA_ikaB"
      },
      "outputs": [],
      "source": [
        "# Predict on train, val and test\n",
        "preds_train_unet3p = unet3p_model.predict(X_train, batch_size=4, verbose=1)\n",
        "preds_val_unet3p = unet3p_model.predict(X_valid, batch_size=4, verbose=1)\n",
        "preds_test_unet3p = unet3p_model.predict(X_test, batch_size=2, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx55rhqmSERo"
      },
      "outputs": [],
      "source": [
        "preds_train_unet3p_1 = preds_train_unet3p[-1]\n",
        "preds_val_unet3p_1 = preds_val_unet3p[-1]\n",
        "preds_test_unet3p_1 = preds_test_unet3p[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94BKoCS0SERo"
      },
      "outputs": [],
      "source": [
        "# Binary predictions\n",
        "preds_train_t_u3 = (preds_train_unet3p_1 > 0.5).astype(np.uint8)\n",
        "preds_val_t_u3 = (preds_val_unet3p_1 > 0.5).astype(np.uint8)\n",
        "preds_test_t_u3 = (preds_test_unet3p_1 > 0.5).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wguWpCmpjyI_"
      },
      "outputs": [],
      "source": [
        "# Plot random sample\n",
        "plot_sample(X_test, y_test, preds_test_unet3p_1, preds_test_t_u3, ix=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SojHSyxvjyI_"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=logdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fszSzqHLkKYn"
      },
      "source": [
        "#### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M24DOQRFkKYn"
      },
      "outputs": [],
      "source": [
        "intersection = np.logical_and(y_test, preds_test_t_u3)\n",
        "union = np.logical_or(y_test, preds_test_t_u3)\n",
        "iou_score = np.sum(intersection) / np.sum(union)\n",
        "print(\"IoU socre is: \", iou_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zP_Fto2kKYn"
      },
      "outputs": [],
      "source": [
        "y_true = y_test.flatten()\n",
        "y_pred_u3 = preds_test_t_u3.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqM78oUwkKYn"
      },
      "outputs": [],
      "source": [
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_true, y_pred_u3)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_true, y_pred_u3)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_true, y_pred_u3)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_true, y_pred_u3)\n",
        "print('F1 score: %f' % f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhS79Pmji7wb"
      },
      "source": [
        "## Attention U-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7LsreatjyxL"
      },
      "source": [
        "use batch_size = 8 > change it in [Data Augmentation](#scrollTo=hoH_AsWezXxy&line=3&uniqifier=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7CrRqPEi-Ao"
      },
      "outputs": [],
      "source": [
        "model_att_path = os.path.join(\"Model Output\", \"Attention U-Net\", \"attention_model_1.h5\")\n",
        "\n",
        "with tf.device('/CPU:0'):\n",
        "    attention_model = load_model(model_att_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_dir = os.path.join(\"Model Output\", \"TransferLearning\")\n",
        "os.makedirs(base_dir, exist_ok=True) \n",
        "logdir = os.path.join(base_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "with tf.device('/CPU:0'):\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.CSVLogger(os.path.join(base_dir, \"attention_amst.csv\"), append=True, separator=';'),\n",
        "        ModelCheckpoint(\n",
        "            filepath=os.path.join(base_dir, \"attention_amst.h5\"),\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1,\n",
        "            save_best_only=True\n",
        "        ),\n",
        "        TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngXnV927jc-6"
      },
      "outputs": [],
      "source": [
        "total_training_sample = np.shape(X_train)[0]\n",
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK4lf98hmE5w"
      },
      "outputs": [],
      "source": [
        "with tf.device('/GPU:0'):\n",
        "    history_att = attention_model.fit(train, steps_per_epoch=round(total_training_sample/batch_size), epochs=75, batch_size=8 , validation_data = (X_valid, y_valid), callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7voaD06EmE5w"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A--UbBV5mE5w"
      },
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "attention_model.evaluate(X_train, y_train, verbose=1, batch_size=8)\n",
        "attention_model.evaluate(X_valid, y_valid, verbose=1, batch_size=8)\n",
        "attention_model.evaluate(X_test, y_test, verbose=1, batch_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmynOdrumE5x"
      },
      "outputs": [],
      "source": [
        "# Predict on train, val and test\n",
        "preds_train_att = attention_model.predict(X_train, batch_size=8, verbose=1)\n",
        "preds_val_att = attention_model.predict(X_valid, batch_size=8, verbose=1)\n",
        "preds_test_att = attention_model.predict(X_test, batch_size=8, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EADg5CA6mE5x"
      },
      "outputs": [],
      "source": [
        "# Binary predictions\n",
        "preds_train_t_att = (preds_train_att > 0.5).astype(np.uint8)\n",
        "preds_val_t_att = (preds_val_att > 0.5).astype(np.uint8)\n",
        "preds_test_t_att = (preds_test_att > 0.5).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rW0wWm1mE5x"
      },
      "outputs": [],
      "source": [
        "# Plot random sample\n",
        "plot_sample(X_test, y_test, preds_test_att, preds_test_t_att, ix=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcPfchMsmE5x"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=logdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwPBzexdmE5x"
      },
      "source": [
        "#### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofgzh4GbmE5x"
      },
      "outputs": [],
      "source": [
        "intersection = np.logical_and(y_test, preds_test_t_att)\n",
        "union = np.logical_or(y_test, preds_test_t_att)\n",
        "iou_score = np.sum(intersection) / np.sum(union)\n",
        "print(\"IoU socre is: \", iou_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ0W9FvamE5y"
      },
      "outputs": [],
      "source": [
        "y_true = y_test.flatten()\n",
        "y_pred_att = preds_test_t_att.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-epod0AmE5y"
      },
      "outputs": [],
      "source": [
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_true, y_pred_att)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_true, y_pred_att)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_true, y_pred_att)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_true, y_pred_att)\n",
        "print('F1 score: %f' % f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jBF_CvB55dQF",
        "eTLsqmdtDccx",
        "rP90CTBozXxy",
        "MGLIPIpdnRtV",
        "JusO64ANoKiI",
        "1x85vjlBfseZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
